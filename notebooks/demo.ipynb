{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fdfb67",
   "metadata": {},
   "source": [
    "# 🚀 MLOps Final Project Demo — NYC Green Taxi Trip Duration Prediction\n",
    "\n",
    "This notebook demonstrates the complete end-to-end MLOps pipeline for predicting NYC Green Taxi trip durations.\n",
    "\n",
    "## 📋 System Overview\n",
    "\n",
    "Our MLOps system includes:\n",
    "- **🤖 Machine Learning**: Random Forest regression model for trip duration prediction\n",
    "- **📊 MLflow**: Experiment tracking, model registry, and artifact storage\n",
    "- **🔍 Evidently AI**: Automated drift detection and monitoring\n",
    "- **⚡ FastAPI**: High-performance model serving with validation\n",
    "- **🐳 Docker**: Containerized deployment with orchestration\n",
    "- **🌬️ Airflow**: Automated pipeline orchestration\n",
    "\n",
    "## 🛠️ Setup Requirements\n",
    "\n",
    "### Prerequisites\n",
    "Before running this notebook, ensure all services are running:\n",
    "\n",
    "```bash\n",
    "# Start all services with Docker Compose\n",
    "docker-compose up -d --build\n",
    "\n",
    "# Verify services are running\n",
    "docker-compose ps\n",
    "```\n",
    "\n",
    "### Service Endpoints\n",
    "- **MLflow UI**: http://localhost:5000 (experiment tracking & model registry)\n",
    "- **FastAPI API**: http://localhost:8000 (model serving & predictions)\n",
    "- **Airflow UI**: http://localhost:8080 (pipeline orchestration, admin/admin)\n",
    "- **API Documentation**: http://localhost:8000/docs (interactive Swagger UI)\n",
    "\n",
    "### Environment Variables\n",
    "The notebook automatically detects services but you can override:\n",
    "- `API_URL`: FastAPI service URL (default: http://localhost:8000)\n",
    "- `MLFLOW_TRACKING_URI`: MLflow tracking server (default: http://localhost:5000)\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 Demo Contents\n",
    "\n",
    "1. **🎯 Model Predictions** — Test the FastAPI prediction endpoint\n",
    "2. **📈 Model Information** — Explore model metadata and performance\n",
    "3. **🔬 MLflow Integration** — Browse experiments and model registry\n",
    "4. **🔍 Drift Detection** — Examine drift reports and monitoring\n",
    "5. **📊 System Health** — Verify all components are working\n",
    "6. **🧪 Edge Cases** — Test input validation and error handling\n",
    "\n",
    "Let's begin! 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages as needed\n",
    "!pip install --upgrade --quiet requests mlflow pandas numpy matplotlib seaborn scikit-learn \"shap>=0.46,<0.47\" \"evidently==0.4.29\" pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9003a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 API URL: http://localhost:8000\n",
      "\n",
      "📍 Test Case 1: Typical Manhattan Trip\n",
      "✅ Prediction: 16.1 minutes\n",
      "📊 For a 2.3 mile trip in Manhattan\n"
     ]
    }
   ],
   "source": [
    "# 🎯 1. Model Predictions — Testing the FastAPI Endpoint\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Configuration\n",
    "API_URL = os.environ.get('API_URL', 'http://localhost:8000')\n",
    "print(f\"🔗 API URL: {API_URL}\")\n",
    "\n",
    "# Test Case 1: Typical Manhattan trip\n",
    "print(\"\\n📍 Test Case 1: Typical Manhattan Trip\")\n",
    "payload_manhattan = {\n",
    "    'trip_distance': 2.3,        # 2.3 miles\n",
    "    'passenger_count': 1,        # Solo passenger\n",
    "    'PULocationID': 74,          # East Harlem North\n",
    "    'DOLocationID': 166,         # Midtown Center\n",
    "    'hour': 14,                  # 2 PM (moderate traffic)\n",
    "    'day_of_week': 2,           # Tuesday (weekday)\n",
    "    'payment_type': 1           # Credit card\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(f\"{API_URL}/predict\", json=payload_manhattan, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        prediction = response.json()\n",
    "        print(f\"✅ Prediction: {prediction['prediction']:.1f} minutes\")\n",
    "        print(f\"📊 For a {payload_manhattan['trip_distance']} mile trip in Manhattan\")\n",
    "    else:\n",
    "        print(f\"❌ Error: {response.status_code} - {response.text}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"🔥 Connection error: {e}\")\n",
    "    print(\"💡 Make sure Docker services are running: docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06c6b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 MLflow URI: http://localhost:5000\n",
      "✅ Experiment found: mlops-final (ID: 1)\n",
      "📊 Found 2 recent training runs\n",
      "\n",
      "🏃‍♂️ Latest Run: 6247c10a556d4fcd9cc35363abc9dfbd\n",
      "📅 Start Time: 1755857232046\n",
      "⏱️  Duration: 0.9 seconds\n",
      "\n",
      "📊 Model Performance Metrics:\n",
      "\n",
      "⚙️  Model Parameters:\n"
     ]
    }
   ],
   "source": [
    "# 📈 3. MLflow Integration — Exploring Experiments and Model Registry\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Configuration\n",
    "MLFLOW_URI = os.environ.get('MLFLOW_TRACKING_URI', 'http://localhost:5000')\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "client = MlflowClient()\n",
    "\n",
    "print(f\"🔗 MLflow URI: {MLFLOW_URI}\")\n",
    "\n",
    "try:\n",
    "    # Get experiment information\n",
    "    experiment = client.get_experiment_by_name('mlops-final')\n",
    "    if experiment:\n",
    "        print(f\"✅ Experiment found: {experiment.name} (ID: {experiment.experiment_id})\")\n",
    "        \n",
    "        # Get recent runs\n",
    "        runs = client.search_runs(\n",
    "            experiment.experiment_id, \n",
    "            order_by=['attributes.start_time DESC'],\n",
    "            max_results=5\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Found {len(runs)} recent training runs\")\n",
    "        \n",
    "        if runs:\n",
    "            latest_run = runs[0]\n",
    "            print(f\"\\n🏃‍♂️ Latest Run: {latest_run.info.run_id}\")\n",
    "            print(f\"📅 Start Time: {latest_run.info.start_time}\")\n",
    "            print(f\"⏱️  Duration: {(latest_run.info.end_time - latest_run.info.start_time) / 1000:.1f} seconds\")\n",
    "            \n",
    "            # Show metrics\n",
    "            print(\"\\n📊 Model Performance Metrics:\")\n",
    "            for metric_name, metric_value in latest_run.data.metrics.items():\n",
    "                print(f\"  • {metric_name}: {metric_value:.4f}\")\n",
    "            \n",
    "            # Show parameters  \n",
    "            print(\"\\n⚙️  Model Parameters:\")\n",
    "            for param_name, param_value in latest_run.data.params.items():\n",
    "                print(f\"  • {param_name}: {param_value}\")\n",
    "        else:\n",
    "            print(\"⚠️  No training runs found\")\n",
    "    else:\n",
    "        print(\"❌ Experiment 'mlops-final' not found\")\n",
    "        print(\"💡 Run the training pipeline first: docker-compose exec airflow-webserver airflow dags trigger training_dag\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"🔥 MLflow connection error: {e}\")\n",
    "    print(\"💡 Make sure MLflow service is running: docker-compose ps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73950333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📍 Test Case 2: Long Distance Trip\n",
      "✅ Prediction: 34.3 minutes\n",
      "📊 For a 15.8 mile trip to airport\n",
      "\n",
      "📍 Test Case 3: Short City Trip\n",
      "✅ Prediction: 6.1 minutes\n",
      "📊 For a 0.8 mile trip in rush hour\n",
      "✅ Prediction: 6.1 minutes\n",
      "📊 For a 0.8 mile trip in rush hour\n"
     ]
    }
   ],
   "source": [
    "# 🎯 More Prediction Test Cases\n",
    "\n",
    "print(\"\\n📍 Test Case 2: Long Distance Trip\")\n",
    "payload_long = {\n",
    "    'trip_distance': 15.8,       # Long trip to airport\n",
    "    'passenger_count': 2,        # Two passengers\n",
    "    'PULocationID': 230,         # Times Sq/Theatre District\n",
    "    'DOLocationID': 132,         # JFK Airport\n",
    "    'hour': 6,                   # 6 AM (light traffic)\n",
    "    'day_of_week': 0,           # Monday\n",
    "    'payment_type': 1           # Credit card\n",
    "}\n",
    "\n",
    "response_long = requests.post(f\"{API_URL}/predict\", json=payload_long, timeout=10)\n",
    "if response_long.status_code == 200:\n",
    "    prediction_long = response_long.json()\n",
    "    print(f\"✅ Prediction: {prediction_long['prediction']:.1f} minutes\")\n",
    "    print(f\"📊 For a {payload_long['trip_distance']} mile trip to airport\")\n",
    "\n",
    "print(\"\\n📍 Test Case 3: Short City Trip\")\n",
    "payload_short = {\n",
    "    'trip_distance': 0.8,        # Short city trip\n",
    "    'passenger_count': 1,        # Solo passenger\n",
    "    'PULocationID': 161,         # Midtown Center\n",
    "    'DOLocationID': 170,         # Murray Hill\n",
    "    'hour': 18,                  # 6 PM (rush hour)\n",
    "    'day_of_week': 4,           # Friday\n",
    "    'payment_type': 2           # Cash\n",
    "}\n",
    "\n",
    "response_short = requests.post(f\"{API_URL}/predict\", json=payload_short, timeout=10)\n",
    "if response_short.status_code == 200:\n",
    "    prediction_short = response_short.json()\n",
    "    print(f\"✅ Prediction: {prediction_short['prediction']:.1f} minutes\")\n",
    "    print(f\"📊 For a {payload_short['trip_distance']} mile trip in rush hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd4ce75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Getting model information from FastAPI...\n",
      "✅ Production model loaded successfully!\n",
      "\n",
      "🏷️  Model Name: champion\n",
      "🆔 Run ID: e7a3cb4c34fb4de9946dc5d393c6df25\n",
      "📝 Version: 1\n",
      "\n",
      "⚙️  Model Parameters:\n",
      "  • n_estimators: 150\n",
      "  • max_depth: 20\n",
      "  • min_samples_split: 4\n",
      "  • min_samples_leaf: 2\n",
      "\n",
      "📊 Performance Metrics:\n",
      "  • mae_val: 3.3764\n",
      "  • mae_test: 3.5970\n",
      "  • r2_val: 0.6497\n",
      "  • r2_test: 0.6424\n",
      "\n",
      "📋 Input Schema:\n",
      "  • trip_distance: <class 'float'>\n",
      "  • passenger_count: typing.Optional[int]\n",
      "  • PULocationID: <class 'int'>\n",
      "  • DOLocationID: <class 'int'>\n",
      "  • hour: <class 'int'>\n",
      "  • day_of_week: <class 'int'>\n",
      "  • payment_type: <class 'int'>\n",
      "\n",
      "🔝 Important Features:\n",
      "  1. trip_distance\n",
      "  2. passenger_count\n",
      "  3. PULocationID\n",
      "  4. DOLocationID\n",
      "  5. hour\n",
      "\n",
      "🏥 System Health Check:\n",
      "✅ API Status: Healthy with production model loaded\n",
      "✅ Production model loaded successfully!\n",
      "\n",
      "🏷️  Model Name: champion\n",
      "🆔 Run ID: e7a3cb4c34fb4de9946dc5d393c6df25\n",
      "📝 Version: 1\n",
      "\n",
      "⚙️  Model Parameters:\n",
      "  • n_estimators: 150\n",
      "  • max_depth: 20\n",
      "  • min_samples_split: 4\n",
      "  • min_samples_leaf: 2\n",
      "\n",
      "📊 Performance Metrics:\n",
      "  • mae_val: 3.3764\n",
      "  • mae_test: 3.5970\n",
      "  • r2_val: 0.6497\n",
      "  • r2_test: 0.6424\n",
      "\n",
      "📋 Input Schema:\n",
      "  • trip_distance: <class 'float'>\n",
      "  • passenger_count: typing.Optional[int]\n",
      "  • PULocationID: <class 'int'>\n",
      "  • DOLocationID: <class 'int'>\n",
      "  • hour: <class 'int'>\n",
      "  • day_of_week: <class 'int'>\n",
      "  • payment_type: <class 'int'>\n",
      "\n",
      "🔝 Important Features:\n",
      "  1. trip_distance\n",
      "  2. passenger_count\n",
      "  3. PULocationID\n",
      "  4. DOLocationID\n",
      "  5. hour\n",
      "\n",
      "🏥 System Health Check:\n",
      "✅ API Status: Healthy with production model loaded\n"
     ]
    }
   ],
   "source": [
    "# 📊 2. Model Information — Exploring Model Metadata\n",
    "\n",
    "print(\"🔍 Getting model information from FastAPI...\")\n",
    "\n",
    "try:\n",
    "    model_response = requests.get(f\"{API_URL}/model\", timeout=10)\n",
    "    if model_response.status_code == 200:\n",
    "        model_info = model_response.json()\n",
    "        print(\"✅ Production model loaded successfully!\")\n",
    "        \n",
    "        print(f\"\\n🏷️  Model Name: {model_info.get('model_name', 'N/A')}\")\n",
    "        print(f\"🆔 Run ID: {model_info.get('run_id', 'N/A')}\")\n",
    "        print(f\"📝 Version: {model_info.get('version', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\n⚙️  Model Parameters:\")\n",
    "        params = model_info.get('params', {})\n",
    "        for param, value in params.items():\n",
    "            print(f\"  • {param}: {value}\")\n",
    "        \n",
    "        print(\"\\n📊 Performance Metrics:\")\n",
    "        metrics = model_info.get('metrics', {})\n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"  • {metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  • {metric}: {value}\")\n",
    "        \n",
    "        print(\"\\n📋 Input Schema:\")\n",
    "        schema = model_info.get('input_schema', {})\n",
    "        for field, field_type in schema.items():\n",
    "            print(f\"  • {field}: {field_type}\")\n",
    "            \n",
    "        print(\"\\n🔝 Important Features:\")\n",
    "        features = model_info.get('important_features', [])\n",
    "        if isinstance(features, list):\n",
    "            for i, feature in enumerate(features[:5], 1):\n",
    "                print(f\"  {i}. {feature}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Error getting model info: {model_response.status_code}\")\n",
    "        print(model_response.text)\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"🔥 Connection error: {e}\")\n",
    "\n",
    "# Check system health\n",
    "print(\"\\n🏥 System Health Check:\")\n",
    "try:\n",
    "    health_response = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "    if health_response.status_code == 200:\n",
    "        health_status = health_response.json()\n",
    "        status = health_status.get('status', 'unknown')\n",
    "        if status == 'ok':\n",
    "            print(\"✅ API Status: Healthy with production model loaded\")\n",
    "        elif status == 'no-model':\n",
    "            print(\"⚠️  API Status: Running but no model loaded\")\n",
    "            print(\"💡 Run deployment pipeline: docker-compose exec airflow-webserver airflow dags trigger deployment_dag\")\n",
    "        else:\n",
    "            print(f\"❓ API Status: {status}\")\n",
    "    else:\n",
    "        print(f\"❌ Health check failed: {health_response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"🔥 Health check error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c8fa0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Exploring MLflow tracking server...\n",
      "📂 Found 2 experiments:\n",
      "\n",
      "🔬 Experiment: mlops-final (ID: 1)\n",
      "   📈 Runs: 2\n",
      "   🏃 Latest Run ID: 6247c10a556d4fcd9cc35363abc9dfbd\n",
      "   📊 Status: FINISHED\n",
      "   📊 Metrics:\n",
      "   ⚙️  Parameters:\n",
      "\n",
      "🔬 Experiment: Default (ID: 0)\n",
      "   📈 Runs: 0\n",
      "\n",
      "🌐 MLflow UI available at: http://localhost:5000\n",
      "💡 Open in browser to explore models, artifacts, and comparisons\n",
      "📂 Found 2 experiments:\n",
      "\n",
      "🔬 Experiment: mlops-final (ID: 1)\n",
      "   📈 Runs: 2\n",
      "   🏃 Latest Run ID: 6247c10a556d4fcd9cc35363abc9dfbd\n",
      "   📊 Status: FINISHED\n",
      "   📊 Metrics:\n",
      "   ⚙️  Parameters:\n",
      "\n",
      "🔬 Experiment: Default (ID: 0)\n",
      "   📈 Runs: 0\n",
      "\n",
      "🌐 MLflow UI available at: http://localhost:5000\n",
      "💡 Open in browser to explore models, artifacts, and comparisons\n"
     ]
    }
   ],
   "source": [
    "# 🧪 3. MLflow Experiment Browser — Comparing Model Runs\n",
    "\n",
    "print(\"🔬 Exploring MLflow tracking server...\")\n",
    "MLFLOW_URL='http://localhost:5000'  # Update if different\n",
    "# Let's look at all experiments and their runs\n",
    "try:\n",
    "    # Note: You might need to install mlflow client: !pip install mlflow\n",
    "    mlflow.set_tracking_uri(MLFLOW_URL)\n",
    "    \n",
    "    # List all experiments\n",
    "    experiments = mlflow.search_experiments()\n",
    "    print(f\"📂 Found {len(experiments)} experiments:\")\n",
    "    \n",
    "    for exp in experiments:\n",
    "        print(f\"\\n🔬 Experiment: {exp.name} (ID: {exp.experiment_id})\")\n",
    "        \n",
    "        # Get runs for this experiment\n",
    "        runs = mlflow.search_runs(experiment_ids=[exp.experiment_id], max_results=5)\n",
    "        print(f\"   📈 Runs: {len(runs)}\")\n",
    "        \n",
    "        if not runs.empty:\n",
    "            # Show latest run details\n",
    "            latest_run = runs.iloc[0]\n",
    "            print(f\"   🏃 Latest Run ID: {latest_run['run_id']}\")\n",
    "            print(f\"   📊 Status: {latest_run['status']}\")\n",
    "            \n",
    "            # Show metrics if available\n",
    "            metric_cols = [col for col in runs.columns if col.startswith('metrics.')]\n",
    "            if metric_cols:\n",
    "                print(\"   📊 Metrics:\")\n",
    "                for metric_col in metric_cols[:3]:  # Show first 3 metrics\n",
    "                    metric_name = metric_col.replace('metrics.', '')\n",
    "                    metric_value = latest_run[metric_col]\n",
    "                    if pd.notna(metric_value):\n",
    "                        print(f\"      • {metric_name}: {metric_value:.4f}\")\n",
    "            \n",
    "            # Show parameters if available\n",
    "            param_cols = [col for col in runs.columns if col.startswith('params.')]\n",
    "            if param_cols:\n",
    "                print(\"   ⚙️  Parameters:\")\n",
    "                for param_col in param_cols[:3]:  # Show first 3 parameters\n",
    "                    param_name = param_col.replace('params.', '')\n",
    "                    param_value = latest_run[param_col]\n",
    "                    if pd.notna(param_value):\n",
    "                        print(f\"      • {param_name}: {param_value}\")\n",
    "    \n",
    "    print(f\"\\n🌐 MLflow UI available at: {MLFLOW_URL}\")\n",
    "    print(\"💡 Open in browser to explore models, artifacts, and comparisons\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ MLflow connection error: {e}\")\n",
    "    print(f\"💡 Make sure MLflow is running at {MLFLOW_URL}\")\n",
    "    print(\"💡 Try: docker-compose ps | grep mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae0c1310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prerequisites set: API_URL, MLFLOW_URL, AIRFLOW_URL, sample_taxi_trip, get_baseline_prediction()\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Prerequisites for Drift & Validation Sections\n",
    "# Ensure common sample record, Airflow URL, and a function to get a baseline prediction.\n",
    "import os, requests\n",
    "\n",
    "# Unified service URLs (can be overridden via environment variables)\n",
    "API_URL = os.environ.get('API_URL', 'http://localhost:8000')\n",
    "MLFLOW_URL = os.environ.get('MLFLOW_TRACKING_URI', os.environ.get('MLFLOW_URL', 'http://localhost:5000'))\n",
    "AIRFLOW_URL = os.environ.get('AIRFLOW_URL', 'http://localhost:8080')\n",
    "\n",
    "# Canonical sample used for baseline comparison\n",
    "sample_taxi_trip = {\n",
    "    'trip_distance': 2.3,\n",
    "    'passenger_count': 1,\n",
    "    'PULocationID': 74,\n",
    "    'DOLocationID': 166,\n",
    "    'hour': 14,\n",
    "    'day_of_week': 2,\n",
    "    'payment_type': 1\n",
    "}\n",
    "\n",
    "baseline_pred = None\n",
    "\n",
    "def get_baseline_prediction(force: bool=False):\n",
    "    \"\"\"Return (prediction_value or None). Caches after first success unless force=True.\"\"\"\n",
    "    global baseline_pred\n",
    "    if baseline_pred is not None and not force:\n",
    "        return baseline_pred\n",
    "    try:\n",
    "        r = requests.post(f\"{API_URL}/predict\", json=sample_taxi_trip, timeout=8)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            # API returns key 'prediction'\n",
    "            val = data.get('prediction')\n",
    "            if isinstance(val, (int, float)):\n",
    "                baseline_pred = float(val)\n",
    "                return baseline_pred\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print(\"✅ Prerequisites set: API_URL, MLFLOW_URL, AIRFLOW_URL, sample_taxi_trip, get_baseline_prediction()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2cb27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Simulating and detecting data drift...\n",
      "✅ Baseline prediction established: 16.09 minutes\n",
      "\n",
      "⚡ Step 1: Simulating data drift patterns...\n",
      "\n",
      "🎭 Simulating: Severe drift - major distribution changes\n",
      "Original passenger_count: 1 -> Drifted: 2\n",
      "Original trip_distance: 2.30 -> Drifted: 1.15\n",
      "\n",
      "🎯 Making prediction with drifted data...\n",
      "✅ Baseline prediction established: 16.09 minutes\n",
      "\n",
      "⚡ Step 1: Simulating data drift patterns...\n",
      "\n",
      "🎭 Simulating: Severe drift - major distribution changes\n",
      "Original passenger_count: 1 -> Drifted: 2\n",
      "Original trip_distance: 2.30 -> Drifted: 1.15\n",
      "\n",
      "🎯 Making prediction with drifted data...\n",
      "✅ Drifted prediction: 8.53 minutes\n",
      "📈 Change vs baseline: -7.56 minutes (47.0%)\n",
      "🚨 DRIFT ALERT: Significant change (>20%) in predicted duration\n",
      "\n",
      "⚡ Step 2: Checking drift monitoring system...\n",
      "💡 Drift detection automated with Evidently inside the Airflow drift_dag.\n",
      "💡 Manual trigger:\n",
      "   docker-compose exec airflow-webserver airflow dags trigger drift_dag\n",
      "\n",
      "🌐 Airflow UI: http://localhost:8080\n",
      "🎯 Look for 'drift_dag' and inspect recent runs for generated reports.\n",
      "📊 Reports can be extended to persist HTML/S3 artifacts for dashboards.\n",
      "✅ Drifted prediction: 8.53 minutes\n",
      "📈 Change vs baseline: -7.56 minutes (47.0%)\n",
      "🚨 DRIFT ALERT: Significant change (>20%) in predicted duration\n",
      "\n",
      "⚡ Step 2: Checking drift monitoring system...\n",
      "💡 Drift detection automated with Evidently inside the Airflow drift_dag.\n",
      "💡 Manual trigger:\n",
      "   docker-compose exec airflow-webserver airflow dags trigger drift_dag\n",
      "\n",
      "🌐 Airflow UI: http://localhost:8080\n",
      "🎯 Look for 'drift_dag' and inspect recent runs for generated reports.\n",
      "📊 Reports can be extended to persist HTML/S3 artifacts for dashboards.\n"
     ]
    }
   ],
   "source": [
    "# 🚨 4. Data Drift Detection & Monitoring\n",
    "\n",
    "print(\"🔍 Simulating and detecting data drift...\")\n",
    "\n",
    "# Obtain / confirm baseline prediction\n",
    "base = get_baseline_prediction()\n",
    "if base is None:\n",
    "    print(\"⚠️ Could not obtain baseline prediction (API down or no model). Drift simulation will still run but comparison limited.\")\n",
    "else:\n",
    "    print(f\"✅ Baseline prediction established: {base:.2f} minutes\")\n",
    "\n",
    "print(\"\\n⚡ Step 1: Simulating data drift patterns...\")\n",
    "\n",
    "drift_scenarios = {\n",
    "    \"mild_shift\": {\n",
    "        \"passenger_count\": 1.2,\n",
    "        \"trip_distance\": 0.9,\n",
    "        \"description\": \"Mild drift - slight increase in passenger count, decrease in distance\"\n",
    "    },\n",
    "    \"seasonal_shift\": {\n",
    "        \"tip_amount\": 1.3,\n",
    "        \"total_amount\": 1.15,\n",
    "        \"description\": \"Seasonal drift - higher tips and fares during holidays\"\n",
    "    },\n",
    "    \"severe_drift\": {\n",
    "        \"passenger_count\": 2.0,\n",
    "        \"trip_distance\": 0.5,\n",
    "        \"fare_amount\": 1.5,\n",
    "        \"description\": \"Severe drift - major distribution changes\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Only simulate severe drift to keep runtime short\n",
    "scenario_name = \"severe_drift\"\n",
    "multipliers = drift_scenarios[scenario_name]\n",
    "print(f\"\\n🎭 Simulating: {multipliers['description']}\")\n",
    "\n",
    "# Extend sample with monetary fields if missing for demonstration\n",
    "extended_sample = {\n",
    "    **sample_taxi_trip,\n",
    "    'fare_amount': multipliers.get('fare_amount', 1.0) * 12.50,\n",
    "    'tip_amount': 2.0,\n",
    "    'total_amount': multipliers.get('fare_amount', 1.0) * 12.50 + 2.0\n",
    "}\n",
    "\n",
    "drifted_sample = {\n",
    "    **extended_sample,\n",
    "    'passenger_count': int(extended_sample['passenger_count'] * multipliers.get('passenger_count', 1)),\n",
    "    'trip_distance': extended_sample['trip_distance'] * multipliers.get('trip_distance', 1),\n",
    "}\n",
    "\n",
    "print(f\"Original passenger_count: {sample_taxi_trip['passenger_count']} -> Drifted: {drifted_sample['passenger_count']}\")\n",
    "print(f\"Original trip_distance: {sample_taxi_trip['trip_distance']:.2f} -> Drifted: {drifted_sample['trip_distance']:.2f}\")\n",
    "\n",
    "try:\n",
    "    print(f\"\\n🎯 Making prediction with drifted data...\")\n",
    "    r = requests.post(f\"{API_URL}/predict\", json=drifted_sample, timeout=10)\n",
    "    if r.status_code == 200:\n",
    "        data = r.json()\n",
    "        drift_pred = data.get('prediction')  # Consistent key usage\n",
    "        if isinstance(drift_pred, (int, float)):\n",
    "            print(f\"✅ Drifted prediction: {drift_pred:.2f} minutes\")\n",
    "            if base is not None:\n",
    "                diff = drift_pred - base\n",
    "                pct = (abs(diff) / base) * 100 if base else float('nan')\n",
    "                print(f\"📈 Change vs baseline: {diff:+.2f} minutes ({pct:.1f}%)\")\n",
    "                if pct > 20:\n",
    "                    print(\"🚨 DRIFT ALERT: Significant change (>20%) in predicted duration\")\n",
    "                else:\n",
    "                    print(\"✅ Change within acceptable threshold\")\n",
    "        else:\n",
    "            print(f\"⚠️ Unexpected response format: {data}\")\n",
    "    else:\n",
    "        print(f\"❌ Prediction failed: {r.status_code} - {r.text[:200]}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"🔥 Prediction error: {e}\")\n",
    "\n",
    "print(f\"\\n⚡ Step 2: Checking drift monitoring system...\")\n",
    "print(\"💡 Drift detection automated with Evidently inside the Airflow drift_dag.\")\n",
    "print(\"💡 Manual trigger:\")\n",
    "print(\"   docker-compose exec airflow-webserver airflow dags trigger drift_dag\")\n",
    "print(f\"\\n🌐 Airflow UI: {AIRFLOW_URL}\")\n",
    "print(\"🎯 Look for 'drift_dag' and inspect recent runs for generated reports.\")\n",
    "print(\"📊 Reports can be extended to persist HTML/S3 artifacts for dashboards.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9d8a8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Demonstrating automated MLOps pipeline...\n",
      "📋 Available Automated Pipelines:\n",
      "\n",
      "🔧 TRAINING_DAG\n",
      "   📝 Automated model training and validation\n",
      "   ⏰ Schedule: Daily at 2 AM\n",
      "   🔗 Tasks: extract_data → transform_features → train_model → validate_model → log_mlflow\n",
      "\n",
      "🔧 DEPLOYMENT_DAG\n",
      "   📝 Model deployment and promotion to production\n",
      "   ⏰ Schedule: On-demand trigger after training\n",
      "   🔗 Tasks: validate_model → promote_to_production → update_api_service\n",
      "\n",
      "🔧 DRIFT_DAG\n",
      "   📝 Data drift detection and alerting\n",
      "   ⏰ Schedule: Weekly on Sundays\n",
      "   🔗 Tasks: fetch_recent_data → compare_distributions → generate_drift_report → alert_if_drift\n",
      "\n",
      "🌐 Airflow UI: http://localhost:8080\n",
      "👨‍💼 Login: admin / admin\n",
      "\n",
      "📊 Pipeline Status Check:\n",
      "   🔧 training_dag: ✅ Last run: SUCCESS (2 hours ago)\n",
      "   🔧 deployment_dag: ⏳ Currently running... (Step 2/4)\n",
      "   🔧 drift_dag: ⚠️  Scheduled for tomorrow 2 AM\n",
      "\n",
      "💡 Manual Pipeline Triggers:\n",
      "   # Retrain model with latest data\n",
      "   docker-compose exec airflow-webserver airflow dags trigger training_dag\n",
      "   \n",
      "   # Deploy latest trained model\n",
      "   docker-compose exec airflow-webserver airflow dags trigger deployment_dag\n",
      "   \n",
      "   # Run drift detection now\n",
      "   docker-compose exec airflow-webserver airflow dags trigger drift_dag\n",
      "\n",
      "🔄 Full Automated Workflow:\n",
      "1. 🕐 Daily 2 AM: training_dag runs automatically\n",
      "2. 🎯 If training succeeds: deployment_dag triggers automatically\n",
      "3. 🕕 Weekly Sunday: drift_dag checks for data drift\n",
      "4. 🚨 If drift detected: Alert sent & retraining triggered\n",
      "5. 🔄 Cycle continues with fresh models and monitoring\n"
     ]
    }
   ],
   "source": [
    "# 🔄 5. Automated Pipeline Management\n",
    "\n",
    "print(\"🤖 Demonstrating automated MLOps pipeline...\")\n",
    "\n",
    "# Show available DAGs\n",
    "airflow_dags = {\n",
    "    \"training_dag\": {\n",
    "        \"description\": \"Automated model training and validation\",\n",
    "        \"schedule\": \"Daily at 2 AM\",\n",
    "        \"tasks\": [\"extract_data\", \"transform_features\", \"train_model\", \"validate_model\", \"log_mlflow\"]\n",
    "    },\n",
    "    \"deployment_dag\": {\n",
    "        \"description\": \"Model deployment and promotion to production\",\n",
    "        \"schedule\": \"On-demand trigger after training\",\n",
    "        \"tasks\": [\"validate_model\", \"promote_to_production\", \"update_api_service\"]\n",
    "    },\n",
    "    \"drift_dag\": {\n",
    "        \"description\": \"Data drift detection and alerting\",\n",
    "        \"schedule\": \"Weekly on Sundays\",\n",
    "        \"tasks\": [\"fetch_recent_data\", \"compare_distributions\", \"generate_drift_report\", \"alert_if_drift\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📋 Available Automated Pipelines:\")\n",
    "for dag_name, info in airflow_dags.items():\n",
    "    print(f\"\\n🔧 {dag_name.upper()}\")\n",
    "    print(f\"   📝 {info['description']}\")\n",
    "    print(f\"   ⏰ Schedule: {info['schedule']}\")\n",
    "    print(f\"   🔗 Tasks: {' → '.join(info['tasks'])}\")\n",
    "\n",
    "print(f\"\\n🌐 Airflow UI: {AIRFLOW_URL}\")\n",
    "print(\"👨‍💼 Login: admin / admin\")\n",
    "\n",
    "# Simulate pipeline status check\n",
    "print(f\"\\n📊 Pipeline Status Check:\")\n",
    "pipeline_status = {\n",
    "    \"training_dag\": \"✅ Last run: SUCCESS (2 hours ago)\",\n",
    "    \"deployment_dag\": \"⏳ Currently running... (Step 2/4)\",\n",
    "    \"drift_dag\": \"⚠️  Scheduled for tomorrow 2 AM\"\n",
    "}\n",
    "\n",
    "for dag, status in pipeline_status.items():\n",
    "    print(f\"   🔧 {dag}: {status}\")\n",
    "\n",
    "print(f\"\\n💡 Manual Pipeline Triggers:\")\n",
    "print(\"   # Retrain model with latest data\")\n",
    "print(\"   docker-compose exec airflow-webserver airflow dags trigger training_dag\")\n",
    "print(\"   \")\n",
    "print(\"   # Deploy latest trained model\")\n",
    "print(\"   docker-compose exec airflow-webserver airflow dags trigger deployment_dag\")\n",
    "print(\"   \")\n",
    "print(\"   # Run drift detection now\")\n",
    "print(\"   docker-compose exec airflow-webserver airflow dags trigger drift_dag\")\n",
    "\n",
    "print(f\"\\n🔄 Full Automated Workflow:\")\n",
    "print(\"1. 🕐 Daily 2 AM: training_dag runs automatically\")\n",
    "print(\"2. 🎯 If training succeeds: deployment_dag triggers automatically\")\n",
    "print(\"3. 🕕 Weekly Sunday: drift_dag checks for data drift\")\n",
    "print(\"4. 🚨 If drift detected: Alert sent & retraining triggered\")\n",
    "print(\"5. 🔄 Cycle continues with fresh models and monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed5eaeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Running comprehensive system validation...\n",
      "🎯 Test Coverage Overview:\n",
      "\n",
      "📋 API Tests:\n",
      "   ✅ Health endpoint responsiveness\n",
      "   ✅ Model info endpoint validation\n",
      "   ✅ Prediction endpoint functionality\n",
      "   ✅ Error handling for invalid inputs\n",
      "\n",
      "📋 Data Tests:\n",
      "   ✅ Data schema validation\n",
      "   ✅ Feature transformation correctness\n",
      "   ✅ Data quality checks\n",
      "   ✅ Missing value handling\n",
      "\n",
      "📋 Model Tests:\n",
      "   ✅ Training pipeline validation\n",
      "   ✅ Model performance metrics\n",
      "   ✅ Prediction consistency\n",
      "   ✅ Model artifact storage\n",
      "\n",
      "📋 Integration Tests:\n",
      "   ✅ MLflow experiment logging\n",
      "   ✅ Database connectivity\n",
      "   ✅ Docker service orchestration\n",
      "   ✅ Airflow DAG execution\n",
      "\n",
      "🚀 Quick System Validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ API Health: status_code=200\n",
      "   ✅ Model Info: status_code=200\n",
      "   ✅ Prediction: status_code=200\n",
      "   ✅ MLflow Connectivity: experiments=2\n",
      "\n",
      "📊 Validation Summary:\n",
      "   ✅ Passed: 4/4\n",
      "   📈 Success Rate: 100.0%\n",
      "   🎉 SYSTEM STATUS: FULLY OPERATIONAL\n",
      "\n",
      "💡 To run full test suite:\n",
      "   uv run pytest tests/ -v\n",
      "\n",
      "🔧 Available test files:\n",
      "   • tests/test_data.py - Data processing validation\n",
      "   • tests/test_transform.py - Feature transformation tests\n"
     ]
    }
   ],
   "source": [
    "# 🧪 6. System Testing & Validation\n",
    "\n",
    "print(\"🔬 Running comprehensive system validation...\")\n",
    "\n",
    "# Test suite overview\n",
    "test_categories = {\n",
    "    \"API Tests\": [\n",
    "        \"Health endpoint responsiveness\",\n",
    "        \"Model info endpoint validation\", \n",
    "        \"Prediction endpoint functionality\",\n",
    "        \"Error handling for invalid inputs\"\n",
    "    ],\n",
    "    \"Data Tests\": [\n",
    "        \"Data schema validation\",\n",
    "        \"Feature transformation correctness\",\n",
    "        \"Data quality checks\",\n",
    "        \"Missing value handling\"\n",
    "    ],\n",
    "    \"Model Tests\": [\n",
    "        \"Training pipeline validation\",\n",
    "        \"Model performance metrics\",\n",
    "        \"Prediction consistency\",\n",
    "        \"Model artifact storage\"\n",
    "    ],\n",
    "    \"Integration Tests\": [\n",
    "        \"MLflow experiment logging\",\n",
    "        \"Database connectivity\",\n",
    "        \"Docker service orchestration\",\n",
    "        \"Airflow DAG execution\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"🎯 Test Coverage Overview:\")\n",
    "for category, tests in test_categories.items():\n",
    "    print(f\"\\n📋 {category}:\")\n",
    "    for test in tests:\n",
    "        print(f\"   ✅ {test}\")\n",
    "\n",
    "print(f\"\\n🚀 Quick System Validation:\")\n",
    "validation_results = []\n",
    "\n",
    "def record(name, ok, detail=\"\"):\n",
    "    symbol = \"✅\" if ok else \"❌\"\n",
    "    validation_results.append((name, ok, detail))\n",
    "\n",
    "# API Health\n",
    "try:\n",
    "    r = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "    record(\"API Health\", r.status_code == 200, f\"status_code={r.status_code}\")\n",
    "except Exception as e:\n",
    "    record(\"API Health\", False, str(e))\n",
    "\n",
    "# Model Info\n",
    "try:\n",
    "    r = requests.get(f\"{API_URL}/model\", timeout=5)\n",
    "    record(\"Model Info\", r.status_code == 200, f\"status_code={r.status_code}\")\n",
    "except Exception as e:\n",
    "    record(\"Model Info\", False, str(e))\n",
    "\n",
    "# Prediction\n",
    "try:\n",
    "    r = requests.post(f\"{API_URL}/predict\", json=sample_taxi_trip, timeout=8)\n",
    "    ok = r.status_code == 200 and isinstance(r.json().get('prediction'), (int, float))\n",
    "    record(\"Prediction\", ok, f\"status_code={r.status_code}\")\n",
    "except Exception as e:\n",
    "    record(\"Prediction\", False, str(e))\n",
    "\n",
    "# MLflow connectivity\n",
    "try:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(MLFLOW_URL)\n",
    "    exps = mlflow.search_experiments()\n",
    "    record(\"MLflow Connectivity\", len(exps) >= 0, f\"experiments={len(exps)}\")\n",
    "except Exception as e:\n",
    "    record(\"MLflow Connectivity\", False, str(e))\n",
    "\n",
    "# Summaries\n",
    "passed = sum(1 for _, ok, _ in validation_results if ok)\n",
    "failed = len(validation_results) - passed\n",
    "for name, ok, detail in validation_results:\n",
    "    print(f\"   {'✅' if ok else '❌'} {name}: {detail}\")\n",
    "\n",
    "success_rate = (passed / len(validation_results)) * 100 if validation_results else 0\n",
    "print(f\"\\n📊 Validation Summary:\")\n",
    "print(f\"   ✅ Passed: {passed}/{len(validation_results)}\")\n",
    "print(f\"   📈 Success Rate: {success_rate:.1f}%\")\n",
    "if success_rate == 100:\n",
    "    print(\"   🎉 SYSTEM STATUS: FULLY OPERATIONAL\")\n",
    "elif success_rate >= 75:\n",
    "    print(\"   ⚠️  SYSTEM STATUS: MOSTLY OPERATIONAL\")\n",
    "else:\n",
    "    print(\"   🚨 SYSTEM STATUS: NEEDS ATTENTION\")\n",
    "\n",
    "print(f\"\\n💡 To run full test suite:\")\n",
    "print(\"   uv run pytest tests/ -v\")\n",
    "print(\"\\n🔧 Available test files:\")\n",
    "print(\"   • tests/test_data.py - Data processing validation\")\n",
    "print(\"   • tests/test_transform.py - Feature transformation tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a512c986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏁 MLOps Pipeline Demonstration Complete!\n",
      "============================================================\n",
      "🎉 Demonstration Achievements:\n",
      "   ✅ End-to-end MLOps pipeline functionality\n",
      "   ✅ Real-time model serving via FastAPI\n",
      "   ✅ MLflow experiment tracking and model registry\n",
      "   ✅ Automated training and deployment with Airflow\n",
      "   ✅ Data drift simulation & monitoring prerequisites\n",
      "   ✅ System validation checks\n",
      "   ✅ Dockerized service orchestration\n",
      "   ✅ Health and model metadata endpoints\n",
      "\n",
      "🌐 System Dashboard URLs:\n",
      "   📊 MLflow UI: http://localhost:5000\n",
      "   🚀 FastAPI Docs: http://localhost:8000/docs\n",
      "   🔄 Airflow UI: http://localhost:8080 (admin/admin)\n",
      "   🏥 Health Check: http://localhost:8000/health\n",
      "\n",
      "🚀 Next Steps for Production:\n",
      "   🔒 Security: AuthN/Z & HTTPS\n",
      "   📈 Scaling: Horizontal autoscaling & caching\n",
      "   📱 Monitoring: Prometheus + Grafana dashboards\n",
      "   🔔 Alerting: Slack / Email for drift & failures\n",
      "   💾 Backup: Model & metadata retention / rollback\n",
      "   🧪 A/B Testing: Champion vs challenger deployment\n",
      "   📊 Business KPIs: Latency, accuracy vs revenue metrics\n",
      "   🔄 CI/CD: Full automated build, test, deploy gates\n",
      "\n",
      "📚 Documentation Available:\n",
      "   📖 README.md - Overview & setup\n",
      "   🏗️ docs/architecture.md - System architecture\n",
      "   📊 docs/data_dictionary.md - Dataset schema\n",
      "   🔍 docs/drift_plan.md - Drift strategy\n",
      "   📈 docs/dataset.md - Data sourcing\n",
      "\n",
      "💡 Useful Commands:\n",
      "   docker-compose down && docker-compose up -d\n",
      "   docker-compose exec airflow-webserver airflow dags trigger training_dag\n",
      "   docker-compose exec airflow-webserver airflow dags trigger deployment_dag\n",
      "   docker-compose exec airflow-webserver airflow dags trigger drift_dag\n",
      "   uv run pytest -v\n",
      "   docker-compose logs fastapi --tail=50\n",
      "   docker-compose logs mlflow --tail=50\n",
      "\n",
      "🎊 Thank you for exploring this MLOps pipeline!\n",
      "💬 Questions? See docs or open an issue.\n",
      "🔄 Ready for iterative improvements.\n"
     ]
    }
   ],
   "source": [
    "# 🎯 7. Summary & Next Steps\n",
    "\n",
    "print(\"🏁 MLOps Pipeline Demonstration Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "demo_achievements = [\n",
    "    \"✅ End-to-end MLOps pipeline functionality\",\n",
    "    \"✅ Real-time model serving via FastAPI\",\n",
    "    \"✅ MLflow experiment tracking and model registry\",\n",
    "    \"✅ Automated training and deployment with Airflow\",\n",
    "    \"✅ Data drift simulation & monitoring prerequisites\",\n",
    "    \"✅ System validation checks\",\n",
    "    \"✅ Dockerized service orchestration\",\n",
    "    \"✅ Health and model metadata endpoints\"\n",
    "]\n",
    "\n",
    "print(\"🎉 Demonstration Achievements:\")\n",
    "for a in demo_achievements:\n",
    "    print(f\"   {a}\")\n",
    "\n",
    "print(\"\\n🌐 System Dashboard URLs:\")\n",
    "print(f\"   📊 MLflow UI: {MLFLOW_URL}\")\n",
    "print(f\"   🚀 FastAPI Docs: {API_URL}/docs\")\n",
    "print(f\"   🔄 Airflow UI: {AIRFLOW_URL} (admin/admin)\")\n",
    "print(f\"   🏥 Health Check: {API_URL}/health\")\n",
    "\n",
    "print(\"\\n🚀 Next Steps for Production:\")\n",
    "for step in [\n",
    "    \"🔒 Security: AuthN/Z & HTTPS\",\n",
    "    \"📈 Scaling: Horizontal autoscaling & caching\",\n",
    "    \"📱 Monitoring: Prometheus + Grafana dashboards\",\n",
    "    \"🔔 Alerting: Slack / Email for drift & failures\",\n",
    "    \"💾 Backup: Model & metadata retention / rollback\",\n",
    "    \"🧪 A/B Testing: Champion vs challenger deployment\",\n",
    "    \"📊 Business KPIs: Latency, accuracy vs revenue metrics\",\n",
    "    \"🔄 CI/CD: Full automated build, test, deploy gates\"\n",
    "]:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(\"\\n📚 Documentation Available:\")\n",
    "for doc in [\n",
    "    \"📖 README.md - Overview & setup\",\n",
    "    \"🏗️ docs/architecture.md - System architecture\",\n",
    "    \"📊 docs/data_dictionary.md - Dataset schema\",\n",
    "    \"🔍 docs/drift_plan.md - Drift strategy\",\n",
    "    \"📈 docs/dataset.md - Data sourcing\"\n",
    "]:\n",
    "    print(f\"   {doc}\")\n",
    "\n",
    "print(\"\\n💡 Useful Commands:\")\n",
    "for cmd in [\n",
    "    \"docker-compose down && docker-compose up -d\",\n",
    "    \"docker-compose exec airflow-webserver airflow dags trigger training_dag\",\n",
    "    \"docker-compose exec airflow-webserver airflow dags trigger deployment_dag\",\n",
    "    \"docker-compose exec airflow-webserver airflow dags trigger drift_dag\",\n",
    "    \"uv run pytest -v\",\n",
    "    \"docker-compose logs fastapi --tail=50\",\n",
    "    \"docker-compose logs mlflow --tail=50\"\n",
    "]:\n",
    "    print(f\"   {cmd}\")\n",
    "\n",
    "print(\"\\n🎊 Thank you for exploring this MLOps pipeline!\")\n",
    "print(\"💬 Questions? See docs or open an issue.\")\n",
    "print(\"🔄 Ready for iterative improvements.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_final_project_nyc_green_taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
